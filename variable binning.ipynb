{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (11,14,18,19,20,37,38,39,40,42,43,46,47,48,49,50,51,52,53,54,55,56,57,59,60,61,62,63,64,67,68) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (34,35,36,56,77,80,83,123,175,176,178,244,245,251,323,654,655,656,657,658,659,660) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "####### PART 0: Load Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "data_folder_path = r'C:\\Users\\xmeng5\\OneDrive - MetLife\\Desktop\\Pet\\Hack_Data\\\\'\n",
    "data_folder_path2 = r'C:\\Users\\xmeng5\\OneDrive - MetLife\\Desktop\\Pet\\data\\\\'\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.options.display.max_seq_items = 2000\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "#internal lead data load and cleanse\n",
    "leads = pd.read_csv(data_folder_path2 + 'PetFirst_Leads_Masked.csv')\n",
    "leads = leads.loc[leads['createdate'].str.len()!=3]\n",
    "leads['createdate'] = pd.to_datetime(leads['createdate'],infer_datetime_format=True)\n",
    "leads['soldpolicy'].replace({'False':0, False:0, True:1, 'True':1},inplace=True)\n",
    "#leads['month'] = leads['createdate'].dt.to_period('M')\n",
    "leads['year'] = pd.DatetimeIndex(leads['createdate']).year\n",
    "\n",
    "leads = leads[leads['createdate'] > '01-01-2019']\n",
    "leads = leads[(leads['pet1age'].astype(str).str[0].str.isdigit())|leads['pet1age'].isnull()]\n",
    "#leads.drop_duplicates(subset=['email','zipcode','year'],inplace=True)\n",
    "#leads.shape, leads['soldpolicy'].mean()\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "leads['petscovered'] = leads['petscovered'].astype(int)\n",
    "\n",
    "#pet1/2/3age can be missing, try derive pet age with petbday. Create pet1/2/3age_combo using derived age to fill missing ages\n",
    "leads[['pet1age','pet2age','pet3age']] = leads[['pet1age','pet2age','pet3age']].astype(float)\n",
    "leads['pet1age_derive'] = pd.to_datetime(leads['createdate'],infer_datetime_format=True) - pd.to_datetime(leads['pet1bday'],infer_datetime_format=True)\n",
    "leads['pet1age_derive'] = leads['pet1age_derive'].dt.days / 365\n",
    "leads['pet2age_derive'] = pd.to_datetime(leads['createdate'],infer_datetime_format=True) - pd.to_datetime(leads['pet2bday'],infer_datetime_format=True)\n",
    "leads['pet2age_derive'] = leads['pet2age_derive'].dt.days / 365\n",
    "leads['pet3age_derive'] = pd.to_datetime(leads['createdate'],infer_datetime_format=True) - pd.to_datetime(leads['pet3bday'],infer_datetime_format=True)\n",
    "leads['pet3age_derive'] = leads['pet3age_derive'].dt.days / 365\n",
    "leads['pet1age_combo'] = np.floor(leads['pet1age'].fillna(leads['pet1age_derive']))\n",
    "leads['pet2age_combo'] = np.floor(leads['pet2age'].fillna(leads['pet2age_derive']))\n",
    "leads['pet3age_combo'] = np.floor(leads['pet3age'].fillna(leads['pet3age_derive']))\n",
    "\n",
    "\n",
    "us_state_abbrev = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'American Samoa': 'AS',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'District of Columbia': 'DC',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Guam': 'GU',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Northern Mariana Islands':'MP',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Puerto Rico': 'PR',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virgin Islands': 'VI',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY'\n",
    "}\n",
    "\n",
    "leads['state'].replace(us_state_abbrev, inplace=True)\n",
    "leads['has_dog'] = leads['pet1species'].isin(['Dog','1'])|leads['pet2species'].isin(['Dog','1'])|leads['pet3species'].isin(['Dog','1'])\n",
    "leads['has_cat'] = leads['pet1species'].isin(['Cat','2'])|leads['pet2species'].isin(['Cat','2'])|leads['pet3species'].isin(['Cat','2'])\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "leads['pet1gender'].replace({'1':'M', 'Male':'M','2':'F',2:'F','Female':'F'},inplace=True)\n",
    "leads['pet2gender'].replace({'1':'M', 'Male':'M','2':'F',2:'F','Female':'F'},inplace=True)\n",
    "leads['pet3gender'].replace({'1':'M', 'Male':'M','2':'F',2:'F','Female':'F'},inplace=True)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "ax_col = ['IN_RECORDNO',\n",
    "          'D_RM_PRTY_HASH_KEY_NEW',\n",
    "          'D_PE_9509_1ST_EDUC_CD',\n",
    "          'D_PE_8610_1ST_GNDR_CD',\n",
    "          'D_PE_8653_ONLINE_PUR_IND',\n",
    "          'D_PE_7744_TRAVEL_US_IND',\n",
    "          'D_PE_8609_HH_MRY_CD',\n",
    "          'D_PE_8622_CHILD_IND',         \n",
    "          'D_PE_7110_ECON_STAB_FIN_CD',\n",
    "          'D_PE_8606_DWELL_OCCUPY_CD',\n",
    "          'D_PE_7737_MAGAZINE_IND',\n",
    "          'D_PE_8616_1ST_AGE_2YR_CD',\n",
    "          'D_PE_1270_PERSONICA_CLUSTER_CD',\n",
    "          'D_PE_1270_PERSONICA_PRECISION',\n",
    "          'ACL_MATCH']\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "#load Acxiom data, enable/disable usecols argument to limit number of columns\n",
    "leads_ax = pd.read_csv(data_folder_path2 + 'ACXIOM_DDAM_ADHOC_TPAPETPROJECTLEA_20201116_0000001.TXT', sep='|')\n",
    "#leads_ax = pd.read_csv(box_base + 'ACXIOM_DDAM_ADHOC_TPAPETPROJECTLEA_20201116_0000001.TXT', sep='|',usecols=ax_col)\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "leads_merge = pd.merge(leads,leads_ax,how='left',left_on='leadid', right_on='IN_RECORDNO').drop_duplicates(subset='leadid')\n",
    "leads_merge['id'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del leads_merge\n",
    "# leads_merge = pd.merge(leads,leads_ax,how='left',left_on='leadid', right_on='IN_RECORDNO').drop_duplicates(subset='leadid')\n",
    "# leads_merge['id'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_merge_copy = leads_merge.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_merge.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mark Data Prep\n",
    "def binVar_append(inpdat, binVar, binValueList, varname_prefix,varname_suffix):   \n",
    "    for num in range(len(binValueList)):\n",
    "        dummy = np.where(inpdat['%s' % binVar] == binValueList[num],1,0)\n",
    "        inpdat['%s' % varname_prefix + varname_suffix[num]] = dummy\n",
    "\n",
    "def Missing_append(inpdat, binVar):   \n",
    "        dummy = np.where(inpdat['%s' % binVar].isnull()==1,1,0)\n",
    "        inpdat['%s' % binVar + '_missing'] = dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plan\n",
    "binVar_append(inpdat = leads_merge, binVar = \"plan\",\n",
    "                          binValueList = [\"Accident & Illness $1000\",\"Accident & Illness $2000\",\n",
    "                                          \"Accident & Illness $3000\",\"Accident & Illness $4000\",\n",
    "                                          \"Accident & Illness $5000\",\"Accident & Illness 30-Day\",\n",
    "                                          \"Premier $1000\",\"Premier $2000\",\"Premier $5000\",\"Standard 30-Day\"],\n",
    "                          varname_prefix = \"Plan_\", \n",
    "                          varname_suffix = [\"AI_1000\",\"AI_2000\",\n",
    "                                          \"AI_3000\",\"AI_4000\",\n",
    "                                          \"AI_5000\",\"AI_30D\",\n",
    "                                          \"P_1000\",\"P_2000\",\"P_5000\",\"S_30D\"])\n",
    "Missing_append(inpdat = leads_merge,binVar = \"plan\")\n",
    "\n",
    "leads_merge = leads_merge.drop(['plan'], axis = 1)\n",
    "\n",
    "#plancopay\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['plancopay'],leads_merge['id']))\n",
    "binVar_append(inpdat = leads_merge, binVar = \"plancopay\",\n",
    "                          binValueList = [\"100%\",\"80%\",\n",
    "                                          \"90%\"],\n",
    "                          varname_prefix = \"PlanCoPay_\", \n",
    "                          varname_suffix = [\"100\",\"80\",\"90\"])\n",
    "Missing_append(inpdat = leads_merge,binVar = \"plancopay\")\n",
    "\n",
    "leads_merge = leads_merge.drop(['plancopay'], axis = 1)\n",
    "\n",
    "#Annual/Monthly\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['annual/monthly'],leads_merge['id']))\n",
    "binVar_append(inpdat = leads_merge, binVar = \"annual/monthly\",\n",
    "                          binValueList = [\"Annual\",\"Monthly\"],\n",
    "                          varname_prefix = \"Pay_\", \n",
    "                          varname_suffix = [\"Annual\",\"Monthly\"])\n",
    "Missing_append(inpdat = leads_merge,binVar = \"annual/monthly\")\n",
    "\n",
    "leads_merge = leads_merge.drop(['annual/monthly'], axis = 1)\n",
    "\n",
    "#PaymentType\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['paymenttype'],leads_merge['id']))\n",
    "binVar_append(inpdat = leads_merge, binVar = \"paymenttype\",\n",
    "                          binValueList = [\"Check(ACH)\",\"Credit Card\",\"PetFirst\",\"VisFin\"],\n",
    "                          varname_prefix = \"Type_\", \n",
    "                          varname_suffix = [\"check\",\"credit\",\"petfirst\",\"visfin\"])\n",
    "Missing_append(inpdat = leads_merge,binVar = \"paymenttype\")\n",
    "\n",
    "leads_merge = leads_merge.drop(['paymenttype'], axis = 1)\n",
    "\n",
    "#EmailDeals\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['emaildeals'],leads_merge['soldpolicy']))\n",
    "binVar_append(inpdat = leads_merge, binVar = \"emaildeals\",\n",
    "                          binValueList = [False,True],\n",
    "                          varname_prefix = \"emaildeal_\", \n",
    "                          varname_suffix = [\"No\",\"Yes\"])\n",
    "leads_merge['EmailDeal'] = np.where(leads_merge['emaildeal_No']==1,0,1)\n",
    "\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['EmailDeal'],leads_merge['id']))\n",
    "leads_merge = leads_merge.drop(['emaildeals','emaildeal_No','emaildeal_Yes'], axis = 1)\n",
    "\n",
    "#PetsCovered\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['petscovered'],leads_merge['id']))\n",
    "binVar_append(inpdat = leads_merge, binVar = \"petscovered\",\n",
    "                          binValueList = [0,1,2,3],\n",
    "                          varname_prefix = \"PetsCovered_\", \n",
    "                          varname_suffix = [\"None\",\"One\",\"Two\",\"Three\"])\n",
    "Missing_append(inpdat = leads_merge,binVar = \"petscovered\")\n",
    "\n",
    "leads_merge = leads_merge.drop(['petscovered'], axis = 1)\n",
    "\n",
    "#PetBreed\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['pet3breed'],leads_merge['id']))\n",
    "leads_merge['breed1lower'] = leads_merge['pet1breed'].str.lower()\n",
    "leads_merge['mix'] = np.where(leads_merge['breed1lower'].str.contains('mix', regex = False) == 1,1,0)\n",
    "leads_merge['domestic'] = np.where(leads_merge['breed1lower'].str.contains('domestic', regex = False) == 1,1,0)\n",
    "leads_merge['Mutt_Code'] = np.where(leads_merge['breed1lower'].isin(['2','54','569','575','576','581','585','586']),1,0)\n",
    "leads_merge['Mutt_Breed'] = leads_merge[['mix', 'domestic','Mutt_Code']].max(axis=1)\n",
    "Missing_append(inpdat = leads_merge,binVar = \"breed1lower\")\n",
    "leads_merge['Pure_Breed'] = np.where((leads_merge['Mutt_Breed'] == 0) & (leads_merge['breed1lower_missing']==0),1,0)\n",
    "leads_merge = leads_merge.drop(['breed1lower','mix','domestic','Mutt_Code','pet1breed','pet2breed','pet3breed'], axis = 1)\n",
    "\n",
    "#pet1gender\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['pet1gender'],leads_merge['id']))\n",
    "binVar_append(inpdat = leads_merge, binVar = \"pet1gender\",\n",
    "                          binValueList = ['F','M'],\n",
    "                          varname_prefix = \"FirstPetGender_\", \n",
    "                          varname_suffix = [\"F\",\"M\"])\n",
    "Missing_append(inpdat = leads_merge,binVar = \"pet1gender\")\n",
    "\n",
    "leads_merge = leads_merge.drop(['pet1gender','pet2gender','pet3gender'], axis = 1)\n",
    "\n",
    "#ishubspotmigrated\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['ishubspotmigrated'],leads_merge['id']))\n",
    "binVar_append(inpdat = leads_merge, binVar = 'ishubspotmigrated',\n",
    "                          binValueList = [False,True],\n",
    "                          varname_prefix = \"HubSpotMigrate_\", \n",
    "                          varname_suffix = [\"No\",\"Yes\"])\n",
    "leads_merge['HubSpotMigrate'] = np.where(leads_merge['HubSpotMigrate_No']==1,0,1)\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['HubSpotMigrate'],leads_merge['id']))\n",
    "\n",
    "leads_merge = leads_merge.drop(['ishubspotmigrated','HubSpotMigrate_No','HubSpotMigrate_Yes'], axis = 1)\n",
    "#iscontactinvalid\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['iscontactinvalid'],leads_merge['id']))\n",
    "binVar_append(inpdat = leads_merge, binVar = 'iscontactinvalid',\n",
    "                          binValueList = [False,True],\n",
    "                          varname_prefix = \"InvalidContact_\", \n",
    "                          varname_suffix = [\"No\",\"Yes\"])\n",
    "leads_merge['InvalidContact'] = np.where(leads_merge['InvalidContact_No']==1,0,1)\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['InvalidContact'],leads_merge['id']))\n",
    "\n",
    "leads_merge = leads_merge.drop(['iscontactinvalid','InvalidContact_No','InvalidContact_Yes'], axis = 1)\n",
    "\n",
    "#Source\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['source'],leads_merge['id']))\n",
    "binVar_append(inpdat = leads_merge, binVar = 'source',\n",
    "                          binValueList = [\"Get Quote Bottom CTA\",\"Consumer\\'s Advocates\",\n",
    "                                          \"Natural Intelligence Desktop\",\"Emily Emails\",\n",
    "                                          \"Brand X Ads\",\"google Search Campaign\",\n",
    "                                          \"Pet Insurance Review Lead Source\",\n",
    "                                          \"Natural Intelligence Mobile\", \"Natural Intelligence_New Site\"],\n",
    "                          varname_prefix = \"Source_\", \n",
    "                          varname_suffix = [\"GetQuote_CTA\",\"ConsumerAdvocates\",\n",
    "                                          \"NatIntelDesktop\",\"EmilyEmails\",\n",
    "                                          \"BrandXAds\",\"googleSearchCampaign\",\n",
    "                                          \"PetInsuranceReview\",\n",
    "                                          \"NaturalIntelligenceMobile\", \"NaturalIntelligenceNewSite\"])\n",
    "Missing_append(inpdat = leads_merge,binVar = \"source\")\n",
    "\n",
    "\n",
    "leads_merge = leads_merge.drop(['source'], axis = 1)\n",
    "\n",
    "#Age\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['pet1age'],leads_merge['id']))\n",
    "leads_merge['Oldest_Pet'] = leads_merge[['pet1age', 'pet2age','pet3age']].max(axis=1)\n",
    "leads_merge['Youngest_Pet'] = leads_merge[['pet1age', 'pet2age','pet3age']].min(axis=1)\n",
    "leads_merge['Average_Pet_Age'] = leads_merge[['pet1age', 'pet2age','pet3age']].mean(axis=1)\n",
    "leads_merge['Median_Pet_Age'] = leads_merge[['pet1age', 'pet2age','pet3age']].median(axis=1)\n",
    "\n",
    "#emailquote\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['emailquote'],leads_merge['id']))\n",
    "binVar_append(inpdat = leads_merge, binVar = 'emailquote',\n",
    "                          binValueList = [False,True],\n",
    "                          varname_prefix = \"EmailQuote_\", \n",
    "                          varname_suffix = [\"No\",\"Yes\"])\n",
    "leads_merge['EmailQuote'] = np.where(leads_merge['EmailQuote_No']==1,0,1)\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['EmailQuote'],leads_merge['id']))\n",
    "\n",
    "leads_merge = leads_merge.drop(['emailquote','EmailQuote_No','EmailQuote_Yes'], axis = 1)\n",
    "\n",
    "#has_cat\n",
    "leads_merge['has_cat'] = np.where(leads_merge['has_cat']==False,0,1)\n",
    "\n",
    "#has_dog\n",
    "leads_merge['has_dog'] = np.where(leads_merge['has_dog']==False,0,1)\n",
    "\n",
    "#Acxiom Fields\n",
    "\n",
    "#EDUCATION\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['D_PE_9509_1ST_EDUC_CD'],leads_merge['id']))\n",
    "binVar_append(inpdat = leads_merge, binVar = \"D_PE_9509_1ST_EDUC_CD\",\n",
    "                          binValueList = [1,2,3,4],\n",
    "                          varname_prefix = \"EDUC_\", \n",
    "                          varname_suffix = [\"HS\",\"COL\",\"GRAD\",\"VOC\"])\n",
    "Missing_append(inpdat = leads_merge,binVar = \"D_PE_9509_1ST_EDUC_CD\")\n",
    "\n",
    "leads_merge = leads_merge.drop(['D_PE_9509_1ST_EDUC_CD'], axis = 1)\n",
    "\n",
    "#PRIMARY PERSON GENDER\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['D_PE_8610_1ST_GNDR_CD'],leads_merge['id']))\n",
    "binVar_append(inpdat = leads_merge, binVar = \"D_PE_8610_1ST_GNDR_CD\",\n",
    "                          binValueList = ['F','M','U'],\n",
    "                          varname_prefix = \"PRIM_GEN_\", \n",
    "                          varname_suffix = [\"F\",\"M\",\"U\"])\n",
    "Missing_append(inpdat = leads_merge,binVar = \"D_PE_8610_1ST_GNDR_CD\")\n",
    "\n",
    "leads_merge = leads_merge.drop(['D_PE_8610_1ST_GNDR_CD'], axis = 1)\n",
    "\n",
    "#ONLINE PURCHASE\n",
    "Missing_append(inpdat = leads_merge,binVar = \"D_PE_8653_ONLINE_PUR_IND\")\n",
    "\n",
    "leads_merge = leads_merge.drop(['D_PE_8653_ONLINE_PUR_IND'], axis = 1)\n",
    "\n",
    "#D_PE_7744_TRAVEL_US_IND (Leave it alone)\n",
    "\n",
    "#MARITAL STATUS\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['D_PE_8609_HH_MRY_CD'],leads_merge['id']))\n",
    "binVar_append(inpdat = leads_merge, binVar = \"D_PE_8609_HH_MRY_CD\",\n",
    "                          binValueList = ['A','B','M','S'],\n",
    "                          varname_prefix = \"M_STATUS_\", \n",
    "                          varname_suffix = [\"INF_MAR\",\"INF_SING\",\"MAR\",\"SING\"])\n",
    "Missing_append(inpdat = leads_merge,binVar = \"D_PE_8609_HH_MRY_CD\")\n",
    "\n",
    "leads_merge = leads_merge.drop(['D_PE_8609_HH_MRY_CD'], axis = 1)\n",
    "\n",
    "#D_PE_8622_CHILD_IND\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['D_PE_8622_CHILD_IND'],leads_merge['id']))\n",
    "binVar_append(inpdat = leads_merge, binVar = \"D_PE_8622_CHILD_IND\",\n",
    "                          binValueList = ['Y','N'],\n",
    "                          varname_prefix = \"KIDS_\", \n",
    "                          varname_suffix = [\"Yes\",\"No\"])\n",
    "Missing_append(inpdat = leads_merge,binVar = \"D_PE_8622_CHILD_IND\")\n",
    "\n",
    "leads_merge = leads_merge.drop(['D_PE_8622_CHILD_IND'], axis = 1)\n",
    "\n",
    "#D_PE_7110_ECON_STAB_FIN_CD (Leave it alone)\n",
    "\n",
    "#HOMEOWNERSHIP\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['D_PE_8606_DWELL_OCCUPY_CD'],leads_merge['id']))\n",
    "binVar_append(inpdat = leads_merge, binVar = \"D_PE_8606_DWELL_OCCUPY_CD\",\n",
    "                          binValueList = ['O','R'],\n",
    "                          varname_prefix = \"HOME_\", \n",
    "                          varname_suffix = [\"OWNER\",\"RENTER\"])\n",
    "Missing_append(inpdat = leads_merge,binVar = \"D_PE_8606_DWELL_OCCUPY_CD\")\n",
    "\n",
    "leads_merge = leads_merge.drop(['D_PE_8606_DWELL_OCCUPY_CD'], axis = 1)\n",
    "\n",
    "#D_PE_7737_MAGAZINE_IND\n",
    "Missing_append(inpdat = leads_merge,binVar = \"D_PE_7737_MAGAZINE_IND\")\n",
    "leads_merge = leads_merge.drop(['D_PE_7737_MAGAZINE_IND'], axis = 1)\n",
    "\n",
    "#D_PE_8616_1ST_AGE_2YR_CD \n",
    "Missing_append(inpdat = leads_merge,binVar = \"D_PE_8616_1ST_AGE_2YR_CD\")\n",
    "\n",
    "#D_PE_1270_PERSONICA_CLUSTER_CD\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['D_PE_1270_PERSONICA_CLUSTER_CD'],leads_merge['id']))\n",
    "binVar_append(inpdat = leads_merge, binVar = \"D_PE_1270_PERSONICA_CLUSTER_CD\",\n",
    "                          binValueList = [13,5,14,26,7,6,10],\n",
    "                          varname_prefix = \"AX_CLUSTER_\", \n",
    "                          varname_suffix = [\"WORKPLAY\",\"ACTIVEINVOLVED\",\"CAREERCENTERED\",\n",
    "                                            \"GETTINGESTABLISHED\",\"ACTIVELIFE\",\"CASUALCOMFORT\",\"CAREERTRAVEL\"])\n",
    "Missing_append(inpdat = leads_merge,binVar = \"D_PE_1270_PERSONICA_CLUSTER_CD\")\n",
    "\n",
    "leads_merge = leads_merge.drop(['D_PE_1270_PERSONICA_CLUSTER_CD'], axis = 1)\n",
    "\n",
    "#TIME VARIABLES\n",
    "\n",
    "leads_merge['DATE'] = pd.to_datetime(leads_merge['createdate'],format = \"%Y-%m-%d\").dt.date\n",
    "\n",
    "leads_merge['month'] = pd.DatetimeIndex(leads_merge['DATE']).month\n",
    "leads_merge['day_of_week'] = pd.DatetimeIndex(leads_merge['DATE']).weekday\n",
    "leads_merge['quarter'] = pd.DatetimeIndex(leads_merge['DATE']).quarter\n",
    "\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['month'],leads_merge['id']))\n",
    "binVar_append(inpdat = leads_merge, binVar = \"month\",\n",
    "                          #binValueList = ['1','2','3','4','5','6','7','8','9','10','11','12'],\n",
    "                          binValueList = [1,2,3,4,5,6,7,8,9,10,11,12],\n",
    "                          varname_prefix = \"Month_\", \n",
    "                          varname_suffix = [\"JAN\",\"FEB\",\"MAR\",\n",
    "                                            \"APR\",\"MAY\",\"JUN\",\"JUL\",\n",
    "                                            \"AUG\",\"SEPT\",\"OCT\",\"NOV\",\"DEC\"])\n",
    "binVar_append(inpdat = leads_merge, binVar = \"day_of_week\",\n",
    "                          #binValueList = ['0','1','2','3','4','5','6'],\n",
    "                          binValueList = [0,1,2,3,4,5,6],\n",
    "                          varname_prefix = \"Day_\", \n",
    "                          varname_suffix = [\"MON\",\"TUES\",\"WED\",\n",
    "                                            \"THURS\",\"FRI\",\"SAT\",\"SUN\"])\n",
    "binVar_append(inpdat = leads_merge, binVar = \"quarter\",\n",
    "                          #binValueList = ['1','2','3','4'],\n",
    "                          binValueList = [1,2,3,4],\n",
    "                          varname_prefix = \"Quarter_\", \n",
    "                          varname_suffix = [\"ONE\",\"TWO\",\"THREE\",\n",
    "                                            \"FOUR\"])\n",
    "\n",
    "leads_merge = leads_merge.drop(['month','day_of_week','quarter','DATE','createdate'], axis = 1)\n",
    "\n",
    "#GEOGRAPHY\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['state'],leads_merge['id']))\n",
    "leads_merge['region'] = np.where(leads_merge['state'].isin(['CA','OR','WA','HI']),'REGION_WEST',\n",
    "                        np.where(leads_merge['state'].isin(['AZ','NM','NV']),'REGION_DESERT',\n",
    "                            np.where(leads_merge['state'].isin(['ID','MT','UT','WY','CO','AK']), 'REGION_ROCKIES', \n",
    "                                     np.where(leads_merge['state'].isin(['ND','SD','KS','NE','MO']), 'REGION_PLAINS', \n",
    "                                         np.where(leads_merge['state'].isin(['OK','TX']), 'REGION_TX',  \n",
    "                                                  np.where(leads_merge['state'].isin(['AR','LA','MS','AL','GA','TN','KY','SC','VA','WV','NC']), 'REGION_SOUTHEAST',\n",
    "                                                         np.where(leads_merge['state'].isin(['FL']), 'REGION_FL', \n",
    "                                                                 np.where(leads_merge['state'].isin(['MN','IA','WI','MI']), 'REGION_UPPER_MW', \n",
    "                                                                     np.where(leads_merge['state'].isin(['IL','IN','OH']), 'REGION_HEARTLAND', \n",
    "                                                                         np.where(leads_merge['state'].isin(['DC','MD','DE','PA','NJ']),'REGION_MID_ATL', \n",
    "                                                                            np.where(leads_merge['state'].isin(['NY']),'REGION_NY', \n",
    "                                                                               np.where(leads_merge['state'].isin(['VT','NH','MA','RI','CT','ME']),'REGION_NEW_ENGLAND','REGION_OTHER'))))))))))))\n",
    "check = pd.DataFrame(pd.crosstab(leads_merge['soldpolicy'],leads_merge['id']))\n",
    "binVar_append(inpdat = leads_merge, binVar = \"region\",\n",
    "                          binValueList = ['REGION_WEST','REGION_DESERT','REGION_ROCKIES','REGION_PLAINS','REGION_TX','REGION_SOUTHEAST','REGION_FL','REGION_UPPER_MW',\n",
    "                                          'REGION_HEARTLAND','REGION_MID_ATL','REGION_NY','REGION_NEW_ENGLAND','REGION_OTHER'],\n",
    "                          varname_prefix = \"Region_\", \n",
    "                          varname_suffix = [\"West\",\"Desert\",\"Rockies\",\n",
    "                                            \"Plains\",\"Texas\",\"SouthEast\",\"Florida\",\n",
    "                                            \"UpperMW\",\"Heartland\",\"MID_ATL\",\"NY\",\"NE\",\"OTHER\"])\n",
    "leads_merge = leads_merge.drop(['state','region'], axis = 1)\n",
    "\n",
    "#soldpolicy\n",
    "leads_merge['soldpolicy'] = np.where(leads_merge['soldpolicy'].isnull()==1,0,leads_merge['soldpolicy'])\n",
    "leads_merge['soldpolicy'] = np.where(leads_merge['soldpolicy']==1,1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del leads_merge_copy\n",
    "del leads_ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM2 = leads_merge.drop(['company','firstname','lastname','phonenumber','email',\n",
    "                        'streetaddress','city','zipcode','quoteid',\n",
    "                        'enrollmentcode','heardoption','completed',\n",
    "                        'pet1name','pet1bday','pet1species','pet2name',\n",
    "                        'pet2bday','pet2species','pet3name','pet3bday',\n",
    "                        'pet3species','email1date','email2date',\n",
    "                        'email3date','unsubscribe','openedemail','numberofclicks',\n",
    "                        'coverage','loyaltycard','workphone','cellphone',\n",
    "                        'streetaddress2','additionalinfo','etsubscriberid',\n",
    "                        'iscontactduplicate','isreported','pet1age','pet2age',\n",
    "                        'pet3age','pet1color','pet2color','pet3color',\n",
    "                        'laststep','pet1size','pet2size','pet3size',\n",
    "                        'discount','issms','leadguidid','year','pet1age_derive',\n",
    "                        'pet2age_derive','pet3age_derive','pet1age_combo',\n",
    "                        'pet2age_combo','pet3age_combo','IN_RECORDNO',\n",
    "                        'D_RM_PRTY_HASH_KEY_NEW','ACL_MATCH','id','D_PE_1270_PERSONICA_PRECISION'],axis = 1)\n",
    "\n",
    "#SPLIT FOR MODELING\n",
    "LM_train, LM_test = train_test_split(LM2, test_size = 0.2, random_state=42)\n",
    "\n",
    "ID_train = LM_train[['leadid']]\n",
    "ID_test = LM_test[['leadid']]\n",
    "\n",
    "X_train = LM_train.drop(['leadid','soldpolicy'], axis = 1)\n",
    "Y_train = LM_train[['soldpolicy']]\n",
    "X_train = X_train.replace(float(\"NaN\"),0)\n",
    "X_test = LM_test.drop(['leadid','soldpolicy'], axis = 1)\n",
    "Y_test = LM_test[['soldpolicy']]\n",
    "X_test = X_test.replace(float(\"NaN\"),0)\n",
    "\n",
    "#MODEL The Data\n",
    "\n",
    "#Gradient Boosting Classifier\n",
    "GBC_obj= GradientBoostingClassifier(max_depth = 6, n_estimators = 150, learning_rate = 0.1)\n",
    "GBC_obj.fit(X_train,Y_train)\n",
    "GBC_val_scores = GBC_obj.predict_proba(X_test)\n",
    "GBC_val_scores2 = GBC_val_scores[:,1]\n",
    "# GBC_roc = roc_auc_score(Y_test, GBC_val_scores2)\n",
    "# print(GBC_roc)\n",
    "\n",
    "# check = pd.DataFrame(GBC_obj.feature_importances_)\n",
    "# check.columns = ['Feature_Importance']\n",
    "# pred_cols = pd.DataFrame(X_train.columns)\n",
    "# pred_cols.columns = ['Predictor']\n",
    "# varimp = pd.concat([pred_cols,check], axis = 1)\n",
    "# varimp['FI_Rank']  = varimp['Feature_Importance'].rank(ascending = False)\n",
    "\n",
    "# #Random Forest\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# forest_obj = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16, n_jobs = -1)\n",
    "# forest_obj.fit(X_train,Y_train)\n",
    "# forest_val_scores = forest_obj.predict_proba(X_test)\n",
    "# forest_val_scores2 = forest_val_scores[:,1]\n",
    "# forest_roc = roc_auc_score(Y_test, forest_val_scores2)\n",
    "# print(forest_roc)\n",
    "\n",
    "# check = pd.DataFrame(forest_obj.feature_importances_)\n",
    "# check.columns = ['Feature_Importance']\n",
    "# pred_cols = pd.DataFrame(X_train.columns)\n",
    "# pred_cols.columns = ['Predictor']\n",
    "# varimp = pd.concat([pred_cols,check], axis = 1)\n",
    "# varimp['FI_Rank']  = varimp['Feature_Importance'].rank(ascending = False)\n",
    "\n",
    "# #Logistic Regression\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# lr_obj = LogisticRegression()\n",
    "# lr_obj.fit(X_train,Y_train)\n",
    "# lr_val_scores = lr_obj.predict_proba(X_test)\n",
    "# lr_val_scores2 = lr_val_scores[:,1]\n",
    "# lr_roc = roc_auc_score(Y_test, lr_val_scores2)\n",
    "# print(lr_roc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
